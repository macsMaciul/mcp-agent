<!DOCTYPE html>
<html lang="en">
  <head>
    <title>HeyGen Streaming API with Voice Activity Detection</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <script src="https://cdn.jsdelivr.net/npm/livekit-client/dist/livekit-client.umd.min.js"></script>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <style>
      .glass-effect {
        background: rgba(255, 255, 255, 0.1);
        backdrop-filter: blur(10px);
        border: 1px solid rgba(255, 255, 255, 0.18);
      }
      
      .audio-level {
        width: 100%;
        height: 30px;
        background: rgba(255, 255, 255, 0.1);
        border-radius: 15px;
        overflow: hidden;
        position: relative;
        margin: 10px 0;
      }
      
      .audio-level-bar {
        height: 100%;
        background: linear-gradient(90deg, #4CAF50, #FFC107, #FF5722);
        width: 0%;
        transition: width 0.1s ease;
        border-radius: 15px;
      }
      
      .audio-level-text {
        position: absolute;
        top: 50%;
        left: 50%;
        transform: translate(-50%, -50%);
        font-size: 12px;
        font-weight: bold;
        text-shadow: 1px 1px 2px rgba(0,0,0,0.5);
        color: white;
      }
      
      .transcript-item {
        margin-bottom: 10px;
        padding: 10px;
        background: rgba(255, 255, 255, 0.05);
        border-radius: 8px;
        border-left: 3px solid rgba(255, 255, 255, 0.3);
        animation: fadeIn 0.3s ease-in;
      }
      
      .transcript-item.processing {
        opacity: 0.7;
        font-style: italic;
      }
      
      .transcript-timestamp {
        font-size: 10px;
        color: rgba(255, 255, 255, 0.6);
        margin-bottom: 5px;
      }
      
      .transcript-text-request {
        font-size: 12px;
        line-height: 1.4;
        color: yellow;
      }
      
      .transcript-text-response {
        font-size: 12px;
        line-height: 1.4;
        color: white;
      }
      
      @keyframes fadeIn {
        from { opacity: 0; transform: translateY(10px); }
        to { opacity: 1; transform: translateY(0); }
      }
      
      @keyframes pulse {
        0% { opacity: 0.7; }
        50% { opacity: 1; }
        100% { opacity: 0.7; }
      }
      
      .recording {
        animation: pulse 1.5s infinite;
      }
    </style>
  </head>

  <body class="bg-gradient-to-br from-purple-600 via-blue-600 to-indigo-700 min-h-screen text-white font-sans">
    <div class="max-w-7xl mx-auto p-5">
      <h1 class="text-3xl font-light text-center mb-8">HeyGen Streaming with Voice Activity Detection</h1>
      
      <div class="grid grid-cols-1 lg:grid-cols-2 gap-6">
        <!-- Left Panel: HeyGen Streaming -->
        <div class="glass-effect p-6 rounded-xl shadow-2xl">
          <h2 class="text-xl font-medium mb-4">üé≠ Avatar Streaming</h2>
          
          <div class="flex flex-wrap gap-2.5 mb-5">
            <input
              id="avatarID"
              type="text"
              placeholder="Avatar ID"
              value="Wayne_20240711"
              class="flex-1 min-w-[200px] p-2 bg-white/20 border border-white/30 rounded-md text-white placeholder-white/70"
            />
            <input
              id="voiceID"
              type="text"
              placeholder="Voice ID"
              class="flex-1 min-w-[200px] p-2 bg-white/20 border border-white/30 rounded-md text-white placeholder-white/70"
            />
          </div>

          <div class="flex flex-wrap gap-2.5 mb-5">
            <button
              id="startBtn"
              class="px-4 py-2 bg-green-500/80 text-white rounded-md hover:bg-green-600 transition-colors disabled:opacity-50 disabled:cursor-not-allowed backdrop-filter backdrop-blur-sm"
            >
              Start Session
            </button>
            <button
              id="closeBtn"
              class="px-4 py-2 bg-red-500/80 text-white rounded-md hover:bg-red-600 transition-colors backdrop-filter backdrop-blur-sm"
            >
              Close Session
            </button>
          </div>

          <div class="flex flex-wrap gap-2.5 mb-5">
            <input
              id="taskInput"
              type="text"
              placeholder="Enter text for avatar to speak"
              class="flex-1 min-w-[200px] p-2 bg-white/20 border border-white/30 rounded-md text-white placeholder-white/70"
            />
            <button
              id="talkBtn"
              class="px-4 py-2 bg-blue-500/80 text-white rounded-md hover:bg-blue-600 transition-colors backdrop-filter backdrop-blur-sm"
            >
              Talk (LLM)
            </button>
            <button
              id="repeatBtn"
              class="px-4 py-2 bg-purple-500/80 text-white rounded-md hover:bg-purple-600 transition-colors backdrop-filter backdrop-blur-sm"
            >
              Repeat
            </button>
          </div>

          <video
            id="mediaElement"
            class="w-full max-h-[300px] border border-white/30 rounded-lg my-5 bg-black/20"
            autoplay
          ></video>
          
          <div
            id="status"
            class="p-2.5 bg-black/20 border border-white/30 rounded-md h-[100px] overflow-y-auto font-mono text-sm"
          ></div>
        </div>

        <!-- Right Panel: Voice Activity Detection -->
        <div class="glass-effect p-6 rounded-xl shadow-2xl">
          <h2 class="text-xl font-medium mb-4">üé§ Voice Activity Detection</h2>
          
          <!-- VAD Settings -->
          <div class="grid grid-cols-2 gap-3 mb-4 text-sm">
            <div>
              <label class="block text-xs font-medium mb-1">Silence Duration (ms)</label>
              <input type="number" id="silenceDuration" value="1500" min="500" max="5000" step="100"
                class="w-full p-2 bg-white/20 border border-white/30 rounded text-white text-xs">
            </div>
            <div>
              <label class="block text-xs font-medium mb-1">Volume Threshold</label>
              <input type="range" id="volumeThreshold" min="0" max="100" value="15"
                class="w-full">
            </div>
            <div>
              <label class="block text-xs font-medium mb-1">Min Recording (ms)</label>
              <input type="number" id="minRecordingDuration" value="1000" min="500" max="5000" step="100"
                class="w-full p-2 bg-white/20 border border-white/30 rounded text-white text-xs">
            </div>
            <div>
              <label class="block text-xs font-medium mb-1">Pre-buffer (ms)</label>
              <input type="number" id="preBufferDuration" value="2000" min="500" max="5000" step="100"
                class="w-full p-2 bg-white/20 border border-white/30 rounded text-white text-xs">
            </div>
          </div>

          <!-- Audio Level Indicator -->
          <div class="audio-level">
            <div class="audio-level-bar" id="audioLevelBar"></div>
            <div class="audio-level-text" id="audioLevelText">Audio Level: 0%</div>
          </div>

          <!-- VAD Status -->
          <div id="vadStatus" class="text-center p-2 bg-black/20 rounded-md mb-4 text-sm">
            Voice detection will start with session
          </div>

          <!-- Stats -->
          <div class="grid grid-cols-3 gap-2 mb-4 text-xs text-center">
            <div>
              <div class="text-lg font-bold" id="transcriptCount">0</div>
              <div>Transcripts</div>
            </div>
            <div>
              <div class="text-lg font-bold" id="totalWords">0</div>
              <div>Words</div>
            </div>
            <div>
              <div class="text-lg font-bold" id="avgProcessingTime">0ms</div>
              <div>Avg Time</div>
            </div>
          </div>

          <!-- Clear Button -->
          <button id="clearBtn" class="w-full px-3 py-2 bg-gray-500/80 text-white rounded-md hover:bg-gray-600 transition-colors backdrop-filter backdrop-blur-sm text-sm mb-4">
            üóëÔ∏è Clear Transcripts
          </button>

          <!-- Transcriptions -->
          <div class="bg-black/20 border border-white/30 rounded-lg p-3 h-[200px] overflow-y-auto">
            <h3 class="text-sm font-medium mb-2">üìù Transcriptions</h3>
            <div id="transcriptsList" class="text-xs">
              Voice segments will appear here...
            </div>
          </div>
        </div>
      </div>
    </div>

    <script>
      // HeyGen Configuration
      const API_CONFIG = {
        apiKey: "YmE5ZDQ2M2ExOTA3NDQ1M2I3ZTU4NTU3ODg1YzcyYzQtMTc0MTM4NDIxNw==",
        serverUrl: "https://api.heygen.com",
      };

      // Global variables for HeyGen
      let sessionInfo = null;
      let room = null;
      let mediaStream = null;
      let webSocket = null;
      let sessionToken = null;

      // Global variables for VAD
      let audioContext = null;
      let mediaRecorder = null;
      let analyser = null;
      let microphone = null;
      let dataArray = null;
      let vadStream = null;
      let isListening = false;
      let isRecording = false;
      let recordingStartTime = null;
      let silenceStartTime = null;
      let audioChunks = [];
      let transcriptCounter = 0;
      let totalWords = 0;
      let processingTimes = [];

      // DOM Elements
      const statusElement = document.getElementById("status");
      const mediaElement = document.getElementById("mediaElement");
      const avatarID = document.getElementById("avatarID");
      const voiceID = document.getElementById("voiceID");
      const taskInput = document.getElementById("taskInput");

      // VAD DOM Elements
      const vadStatus = document.getElementById("vadStatus");
      const transcriptsList = document.getElementById("transcriptsList");
      const silenceDuration = document.getElementById("silenceDuration");
      const volumeThreshold = document.getElementById("volumeThreshold");
      const minRecordingDuration = document.getElementById("minRecordingDuration");
      const preBufferDuration = document.getElementById("preBufferDuration");
      const audioLevelBar = document.getElementById("audioLevelBar");
      const audioLevelText = document.getElementById("audioLevelText");
      const transcriptCountEl = document.getElementById("transcriptCount");
      const totalWordsEl = document.getElementById("totalWords");
      const avgProcessingTimeEl = document.getElementById("avgProcessingTime");

      // Helper function to update status
      function updateStatus(message) {
        const timestamp = new Date().toLocaleTimeString();
        statusElement.innerHTML += `[${timestamp}] ${message}<br>`;
        statusElement.scrollTop = statusElement.scrollHeight;
      }

      // VAD Helper Functions
      function updateVADStatus(message) {
        vadStatus.innerHTML = message;
      }

      async function processInput(text) {
        try {
          const response = await fetch('/process', {
            method: 'POST',
            body: text
          });

          if (!response.ok) {
            throw new Error(`HTTP error! status: ${response.status}`);
          }
          
          transcriptCounter++;
          const transcriptId = `transcript-${transcriptCounter}`;
          vadDetector.addTranscriptToUI(transcriptId, 'üîÑ Processing...', true);

          const result = await response.json();
          const processingTime = 0;
            
          if (result.success && result.text && result.text.trim()) {
            vadDetector.updateTranscriptInUI(transcriptId, result.requestText, result.text);
            vadDetector.updateStats(result.text, processingTime);
          }
        } catch (error) {
            console.error('Error sending text:', error);
        }
      }

      // Voice Activity Detection Class
      class VoiceActivityDetector {
        async startListening() {
          try {
            // Get microphone access
            vadStream = await navigator.mediaDevices.getUserMedia({
              audio: {
                sampleRate: 16000,
                channelCount: 1,
                echoCancellation: true,
                noiseSuppression: true
              }
            });
            
            // Setup audio context for volume analysis
            audioContext = new (window.AudioContext || window.webkitAudioContext)();
            analyser = audioContext.createAnalyser();
            microphone = audioContext.createMediaStreamSource(vadStream);
            
            analyser.fftSize = 256;
            analyser.smoothingTimeConstant = 0.3;
            microphone.connect(analyser);
            
            dataArray = new Uint8Array(analyser.frequencyBinCount);
            
            isListening = true;
            updateVADStatus('üëÇ Listening for voice...');
            
            // Start the voice activity detection loop
            this.detectVoiceActivity();
            
            console.log('Voice activity detection started');
            
          } catch (error) {
            console.error('Error starting voice detection:', error);
            updateVADStatus('‚ùå Error: Could not access microphone');
          }
        }
        
        detectVoiceActivity() {
          if (!isListening) return;
          
          // Analyze audio levels
          analyser.getByteFrequencyData(dataArray);
          
          // Calculate average volume
          const average = dataArray.reduce((a, b) => a + b) / dataArray.length;
          const volumePercent = Math.round((average / 255) * 100);
          
          // Update visual feedback
          audioLevelBar.style.width = volumePercent + '%';
          audioLevelText.textContent = `Audio Level: ${volumePercent}%`;
          
          const threshold = parseInt(volumeThreshold.value);
          const silenceDurationMs = parseInt(silenceDuration.value);
          const minRecordingMs = parseInt(minRecordingDuration.value);
          
          const now = Date.now();
          
          if (volumePercent > threshold) {
            // Voice detected
            silenceStartTime = null;
            
            if (!isRecording) {
              // Start a new recording session
              this.startRecording();
            }
          } else {
            // Silence detected
            if (isRecording) {
              if (!silenceStartTime) {
                silenceStartTime = now;
              } else if (now - silenceStartTime > silenceDurationMs) {
                // Stop recording after silence duration
                const recordingDuration = now - recordingStartTime;
                if (recordingDuration >= minRecordingMs) {
                  this.stopRecording();
                } else {
                  console.log(`Recording too short (${recordingDuration}ms), continuing...`);
                  silenceStartTime = null; // Reset silence timer
                }
              }
            }
          }
          
          // Continue monitoring
          requestAnimationFrame(() => this.detectVoiceActivity());
        }
        
        createMediaRecorder() {
          // Create a fresh MediaRecorder for each recording session
          let mimeType = 'audio/webm;codecs=opus';
          if (!MediaRecorder.isTypeSupported(mimeType)) {
            mimeType = 'audio/webm';
          }
          
          const recorder = new MediaRecorder(vadStream, { mimeType });
          
          recorder.ondataavailable = (event) => {
            if (event.data.size > 0) {
              audioChunks.push(event.data);
            }
          };
          
          recorder.onstop = () => {
            // Process the recording when it stops
            this.processRecording();
          };
          
          return recorder;
        }
        
        startRecording() {
          if (isListening && !isRecording) {
            // Create a new MediaRecorder for this session
            mediaRecorder = this.createMediaRecorder();
            audioChunks = [];
            
            mediaRecorder.start();
            isRecording = true;
            recordingStartTime = Date.now();
            updateVADStatus('üî¥ Recording voice...');
            console.log('Started recording voice segment');
          }
        }
        
        stopRecording() {
          if (mediaRecorder && isRecording) {
            mediaRecorder.stop();
            isRecording = false;
            silenceStartTime = null;
            updateVADStatus('‚è≥ Processing speech...');
            console.log('Stopped recording, processing...');
          }
        }

        async processRecording() {
          if (audioChunks.length === 0) {
            console.log('No audio data to process');
            return;
          }
          
          const audioBlob = new Blob(audioChunks, { type: mediaRecorder.mimeType });
          const startTime = Date.now();
          
          transcriptCounter++;
          const transcriptId = `transcript-${transcriptCounter}`;
          
          // Add processing indicator
          this.addTranscriptToUI(transcriptId, 'üîÑ Processing...', true);
          
          const formData = new FormData();
          formData.append('audio', audioBlob, 'voice-segment.webm');
          formData.append('transcriptId', transcriptId);
          
          try {
            const response = await fetch('/transcribe', {
              method: 'POST',
              body: formData
            });
            
            if (!response.ok) {
              throw new Error(`HTTP error! status: ${response.status}`);
            }
            
            const result = await response.json();
            const processingTime = Date.now() - startTime;
            
            if (result.success && result.text && result.text.trim()) {
              this.updateTranscriptInUI(transcriptId, result.requestText, result.text);
              this.updateStats(result.text, processingTime);
              
              // Send transcribed text to HeyGen avatar
              if (sessionInfo && result.text.trim()) {
                sendText(result.text.trim(), "repeat");
              }
            } else {
              this.removeTranscriptFromUI(transcriptId);
              transcriptCounter--; // Don't count failed transcriptions
              console.log('Empty or failed transcription');
            }
            
            // Resume listening status
            if (isListening) {
              updateVADStatus('üëÇ Listening for voice...');
            }
            
          } catch (error) {
            console.error('Error transcribing audio:', error);
            this.updateTranscriptInUI(transcriptId, "", '‚ùå Error processing audio');
          }
        }
        
        addTranscriptToUI(transcriptId, text, processing = false) {
          const transcriptDiv = document.createElement('div');
          transcriptDiv.className = `transcript-item ${processing ? 'processing' : ''}`;
          transcriptDiv.id = transcriptId;
          
          const timestamp = new Date().toLocaleTimeString();
          transcriptDiv.innerHTML = `
            <div class="transcript-timestamp">${timestamp}</div>
            <div class="transcript-text-request">${text}</div>
            <div class="transcript-text-response"></div>
          `;
          
          transcriptsList.appendChild(transcriptDiv);
          requestAnimationFrame(() => {
            transcriptsList.scrollTop = transcriptsList.scrollHeight;
          });
        }
        
        updateTranscriptInUI(transcriptId, requestText, responseText) {
          const transcriptEl = document.getElementById(transcriptId);
          if (transcriptEl) {
            transcriptEl.classList.remove('processing');
            const textEl1 = transcriptEl.querySelector('.transcript-text-request');
            if (textEl1) {
              textEl1.textContent = requestText;
            }
            const textEl2 = transcriptEl.querySelector('.transcript-text-response');
            if (textEl2) {
              textEl2.textContent = responseText;
            }
            textEl2.scrollIntoView({ behavior: 'smooth' });
          }
          requestAnimationFrame(() => {
            transcriptsList.scrollTop = transcriptsList.scrollHeight;
          });
        }
        
        removeTranscriptFromUI(transcriptId) {
          const transcriptEl = document.getElementById(transcriptId);
          if (transcriptEl) {
            transcriptEl.remove();
          }
        }
        
        updateStats(text, processingTime) {
          processingTimes.push(processingTime);
          totalWords += text.split(' ').length;
          
          transcriptCountEl.textContent = transcriptCounter;
          totalWordsEl.textContent = totalWords;
          
          const avgTime = processingTimes.reduce((a, b) => a + b, 0) / processingTimes.length;
          avgProcessingTimeEl.textContent = Math.round(avgTime) + 'ms';
        }
        
        stopListening() {
          isListening = false;
          
          if (isRecording) {
            this.stopRecording();
          }
          
          if (audioContext) {
            audioContext.close();
            audioContext = null;
          }
          
          if (vadStream) {
            vadStream.getTracks().forEach(track => track.stop());
            vadStream = null;
          }
          
          updateVADStatus('‚èπÔ∏è Voice detection stopped');
          audioLevelBar.style.width = '0%';
          audioLevelText.textContent = 'Audio Level: 0%';
          
          console.log('Voice activity detection stopped');
        }
        
        clearTranscripts() {
          transcriptsList.innerHTML = 'Voice segments will appear here...';
          transcriptCounter = 0;
          totalWords = 0;
          processingTimes = [];
          
          transcriptCountEl.textContent = '0';
          totalWordsEl.textContent = '0';
          avgProcessingTimeEl.textContent = '0ms';
        }
      }

      const vadDetector = new VoiceActivityDetector();

      // HeyGen Functions
      async function getSessionToken() {
        const response = await fetch(
          `${API_CONFIG.serverUrl}/v1/streaming.create_token`,
          {
            method: "POST",
            headers: {
              "Content-Type": "application/json",
              "X-Api-Key": API_CONFIG.apiKey,
            },
          }
        );

        const data = await response.json();
        sessionToken = data.data.token;
        updateStatus("Session token obtained");
      }

      async function connectWebSocket(sessionId) {
        const params = new URLSearchParams({
          session_id: sessionId,
          session_token: sessionToken,
          silence_response: false,
          opening_text: "Hello, how can I help you?",
          stt_language: "en",
        });

        const wsUrl = `wss://${
          new URL(API_CONFIG.serverUrl).hostname
        }/v1/ws/streaming.chat?${params}`;

        webSocket = new WebSocket(wsUrl);

        webSocket.addEventListener("message", (event) => {
          const eventData = JSON.parse(event.data);
          console.log("Raw WebSocket event:", eventData);
        });
      }

      async function createNewSession() {
        if (!sessionToken) {
          await getSessionToken();
        }

        const response = await fetch(
          `${API_CONFIG.serverUrl}/v1/streaming.new`,
          {
            method: "POST",
            headers: {
              "Content-Type": "application/json",
              Authorization: `Bearer ${sessionToken}`,
            },
            body: JSON.stringify({
              quality: "high",
              avatar_name: avatarID.value,
              voice: {
                voice_id: voiceID.value,
                rate: 1.0,
              },
              version: "v2",
              video_encoding: "H264",
            }),
          }
        );

        const data = await response.json();
        sessionInfo = data.data;

        room = new LivekitClient.Room({
          adaptiveStream: true,
          dynacast: true,
          videoCaptureDefaults: {
            resolution: LivekitClient.VideoPresets.h720.resolution,
          },
        });

        room.on(LivekitClient.RoomEvent.DataReceived, (message) => {
          const data = new TextDecoder().decode(message);
          console.log("Room message:", JSON.parse(data));
        });

        mediaStream = new MediaStream();
        room.on(LivekitClient.RoomEvent.TrackSubscribed, (track) => {
          if (track.kind === "video" || track.kind === "audio") {
            mediaStream.addTrack(track.mediaStreamTrack);
            if (
              mediaStream.getVideoTracks().length > 0 &&
              mediaStream.getAudioTracks().length > 0
            ) {
              mediaElement.srcObject = mediaStream;
              updateStatus("Media stream ready");
            }
          }
        });

        room.on(LivekitClient.RoomEvent.TrackUnsubscribed, (track) => {
          const mediaTrack = track.mediaStreamTrack;
          if (mediaTrack) {
            mediaStream.removeTrack(mediaTrack);
          }
        });

        room.on(LivekitClient.RoomEvent.Disconnected, (reason) => {
          updateStatus(`Room disconnected: ${reason}`);
        });

        await room.prepareConnection(sessionInfo.url, sessionInfo.access_token);
        updateStatus("Connection prepared");

        await connectWebSocket(sessionInfo.session_id);

        updateStatus("Session created successfully");
      }

      async function startStreamingSession() {
        /*
        const startResponse = await fetch(
          `${API_CONFIG.serverUrl}/v1/streaming.start`,
          {
            method: "POST",
            headers: {
              "Content-Type": "application/json",
              Authorization: `Bearer ${sessionToken}`,
            },
            body: JSON.stringify({
              session_id: sessionInfo.session_id,
            }),
          }
        );

        await room.connect(sessionInfo.url, sessionInfo.access_token);
        updateStatus("Connected to room");

        document.querySelector("#startBtn").disabled = true;
        updateStatus("Streaming started successfully");
        */
        // Start voice activity detection
        await vadDetector.startListening();
      }

      async function sendText(text, taskType = "talk") {
        if (!sessionInfo) {
          updateStatus("No active session");
          return;
        }

        const response = await fetch(
          `${API_CONFIG.serverUrl}/v1/streaming.task`,
          {
            method: "POST",
            headers: {
              "Content-Type": "application/json",
              Authorization: `Bearer ${sessionToken}`,
            },
            body: JSON.stringify({
              session_id: sessionInfo.session_id,
              text: text,
              task_type: taskType,
            }),
          }
        );

        updateStatus(`Sent text (${taskType}): ${text}`);
      }

      async function closeSession() {
        // Stop voice activity detection first
        vadDetector.stopListening();

        if (!sessionInfo) {
          updateStatus("No active session");
          return;
        }


        const response = await fetch(
          `${API_CONFIG.serverUrl}/v1/streaming.stop`,
          {
            method: "POST",
            headers: {
              "Content-Type": "application/json",
              Authorization: `Bearer ${sessionToken}`,
            },
            body: JSON.stringify({
              session_id: sessionInfo.session_id,
            }),
          }
        );

        if (webSocket) {
          webSocket.close();
        }
        if (room) {
          room.disconnect();
        }

        mediaElement.srcObject = null;
        sessionInfo = null;
        room = null;
        mediaStream = null;
        sessionToken = null;
        document.querySelector("#startBtn").disabled = false;

        updateStatus("Session closed");
      }

      // Event Listeners
      document.querySelector("#startBtn").addEventListener("click", async () => {
        //await createNewSession();
        await startStreamingSession();
      });

      document.querySelector("#closeBtn").addEventListener("click", closeSession);

      document.querySelector("#talkBtn").addEventListener("click", () => {
        const text = taskInput.value.trim();
        if (text) {
          sendText(text, "talk");
          taskInput.value = "";
        }
        processInput(text);
      });

      document.querySelector("#repeatBtn").addEventListener("click", () => {
        const text = taskInput.value.trim();
        if (text) {
          sendText(text, "repeat");
          taskInput.value = "";
        }
      });

      document.querySelector("#clearBtn").addEventListener("click", () => {
        vadDetector.clearTranscripts();
      });
    </script>
  </body>
</html>